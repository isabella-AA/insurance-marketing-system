import os
import time
import requests
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from .base_agent import BaseAgent
from utils.vector_utils import VectorUtils
from utils.weibo_client import WeiboClient
from dotenv import load_dotenv

load_dotenv('config/.env')

class MaterialCollectorAgent(BaseAgent):
    """
    ç´ ææ”¶é›†æ™ºèƒ½ä½“
    è´Ÿè´£ä¸ºé£é™©äº‹ä»¶æ”¶é›†ç›¸å…³çš„è¡¥å……ç´ æå’Œå›¾ç‰‡
    """
    
    def __init__(self):
        super().__init__("MaterialCollector")
        
        # é…ç½®å‚æ•°
        self.index_name = os.getenv("HOT_EVENT_INDEX", "hoteventdb")
        self.batch_size = int(os.getenv("MATERIAL_COLLECTOR_BATCH_SIZE", 3))
        self.max_search_results = int(os.getenv("MAX_SEARCH_RESULTS", 10))
        self.similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", 0.8))
        self.request_timeout = int(os.getenv("REQUEST_TIMEOUT", 15))
        
        # åˆå§‹åŒ–å‘é‡å·¥å…·å’Œå¾®åšå®¢æˆ·ç«¯
        self.vector_utils = VectorUtils()
        self.weibo_client = WeiboClient()
        
        # éªŒè¯å¾®åšCookie
        cookie_info = self.weibo_client.get_cookie_info()
        if cookie_info['has_cookie'] and cookie_info['is_valid']:
            self.logger.info("âœ… å¾®åšCookieéªŒè¯æˆåŠŸï¼Œå¯ä»¥è·å–è¯¦ç»†å†…å®¹")
        else:
            self.logger.warning("âš ï¸ å¾®åšCookieæ— æ•ˆæˆ–æœªé…ç½®ï¼Œåªèƒ½è·å–åŸºç¡€æœç´¢ç»“æœ")
        
        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # æœç´¢å¼•æ“é…ç½®
        self.search_engines = {
            'weibo': self._search_weibo,
            'baidu': self._search_baidu,
            'sogou': self._search_sogou
        }
        
        self.logger.info(f"âœ… ç´ ææ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
    
    def run_once(self) -> str:
        """
        æ‰§è¡Œä¸€æ¬¡ç´ ææ”¶é›†ä»»åŠ¡
        
        Returns:
            å¤„ç†ç»“æœæè¿°
        """
        # è·å–å¾…æ”¶é›†ç´ æçš„äº‹ä»¶
        events = self._fetch_pending_events()
        
        if not events:
            self.logger.info("âš ï¸ æš‚æ— å¾…æ”¶é›†ç´ æçš„äº‹ä»¶")
            return "æ— å¾…å¤„ç†äº‹ä»¶"
        
        # å¤„ç†äº‹ä»¶
        success_count = 0
        total_count = len(events)
        
        for event in events:
            try:
                if self._collect_materials_for_event(event):
                    success_count += 1
                    
            except Exception as e:
                self.logger.error(f"âŒ æ”¶é›†ç´ æå¤±è´¥: {event.get('title', 'Unknown')}, {e}")
        
        result = f"ç´ ææ”¶é›†å®Œæˆ: {success_count}/{total_count} æˆåŠŸ"
        self.logger.info(f"ğŸ“Š {result}")
        return result
    
    def _fetch_pending_events(self) -> List[Dict[str, Any]]:
        """
        è·å–å¾…æ”¶é›†ç´ æçš„äº‹ä»¶
        
        Returns:
            äº‹ä»¶åˆ—è¡¨
        """
        try:
            query = {
                "bool": {
                    "must": [
                        {"term": {"risk_analyzed": True}},
                        {"exists": {"field": "risk_element"}}
                    ],
                    "must_not": [
                        {"term": {"material_collected": True}}
                    ]
                }
            }
            
            events = self.es.search(
                index=self.index_name,
                query=query,
                size=self.batch_size
            )
            
            self.logger.debug(f"ğŸ” è·å–åˆ° {len(events)} ä¸ªå¾…æ”¶é›†ç´ æçš„äº‹ä»¶")
            return events
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¾…æ”¶é›†äº‹ä»¶å¤±è´¥: {e}")
    def _search_baidu(self, keyword: str) -> List[Dict[str, str]]:
           pass
    def _collect_materials_for_event(self, event: Dict[str, Any]) -> bool:
        """
        ä¸ºå•ä¸ªäº‹ä»¶æ”¶é›†ç´ æ
        
        Args:
            event: äº‹ä»¶æ•°æ®
            
        Returns:
            æ˜¯å¦æ”¶é›†æˆåŠŸ
        """
        title = event.get("title", "")
        content = event.get("content", "")
        event_id = event.get("_id")
        
        if not title:
            self.logger.warning(f"âš ï¸ äº‹ä»¶æ ‡é¢˜ä¸ºç©º: {event_id}")
            return False
        
        self.logger.info(f"ğŸ” æ­£åœ¨æ”¶é›†ç´ æ: {title[:50]}...")
        
        # å…ˆæ ‡è®°ä¸ºæ­£åœ¨å¤„ç†ï¼Œé¿å…é‡å¤å¤„ç†
        self._mark_processing(event_id)
        
        try:
            # æ”¶é›†ç½‘é¡µç´ æ
            web_materials = self._collect_web_materials(title, content)
            
            # æ”¶é›†å›¾ç‰‡ç´ æ
            image_materials = self._collect_image_materials(title)
            
            # æ•´åˆç´ æ
            all_materials = {
                "texts": web_materials,
                "image_urls": image_materials,
                "collected_at": time.time()
            }
            
            # æ›´æ–°äº‹ä»¶è®°å½•
            return self._update_event_materials(event_id, all_materials)
            
        except Exception as e:
            self.logger.error(f"âŒ æ”¶é›†ç´ æå¼‚å¸¸: {title}, {e}")
            # æ ‡è®°ä¸ºå¤±è´¥ï¼Œä½†ä¸é˜»æ­¢åç»­å¤„ç†
            self._mark_failed(event_id)
            return False
    
    def _collect_web_materials(self, title: str, content: str) -> List[str]:
        """
        æ”¶é›†ç½‘é¡µæ–‡æœ¬ç´ æ
        
        Args:
            title: äº‹ä»¶æ ‡é¢˜
            content: äº‹ä»¶å†…å®¹
            
        Returns:
            ç›¸å…³æ–‡æœ¬åˆ—è¡¨
        """
        all_texts = []
        
        # ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“
        for engine_name, search_func in self.search_engines.items():
            try:
                self.logger.debug(f"ğŸ” ä½¿ç”¨ {engine_name} æœç´¢ç›¸å…³å†…å®¹")
                search_results = search_func(title)
                
                for result in search_results[:3]:  # æ¯ä¸ªå¼•æ“æœ€å¤šå–3ä¸ªç»“æœ
                    text_content = self._extract_text_from_url(result.get('url', ''))
                    if text_content:
                        # æ£€æŸ¥ç›¸å…³æ€§
                        if self._is_relevant_content(title, text_content):
                            all_texts.append({
                                'content': text_content,
                                'source': result.get('url', ''),
                                'engine': engine_name
                            })
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ {engine_name} æœç´¢å¤±è´¥: {e}")
                continue
        
        # å»é‡å’Œç­›é€‰
        unique_texts = self._deduplicate_texts(all_texts)
        
        self.logger.info(f"ğŸ“ æ”¶é›†åˆ° {len(unique_texts)} æ¡æ–‡æœ¬ç´ æ")
        return unique_texts[:5]  # æœ€å¤šä¿ç•™5æ¡
    
    def _collect_image_materials(self, title: str) -> List[str]:
        """
        æ”¶é›†å›¾ç‰‡ç´ æ
        
        Args:
            title: äº‹ä»¶æ ‡é¢˜
            
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        image_urls = []
        
        try:
            # ä½¿ç”¨å¾®åšæœç´¢å›¾ç‰‡
            weibo_images = self._search_weibo_images(title)
            image_urls.extend(weibo_images)
            
            # å¯ä»¥æ·»åŠ å…¶ä»–å›¾ç‰‡æº
            # baidu_images = self._search_baidu_images(title)
            # image_urls.extend(baidu_images)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å›¾ç‰‡æ”¶é›†å¤±è´¥: {e}")
        
        # å»é‡å’ŒéªŒè¯
        valid_images = self._validate_image_urls(image_urls)
        
        self.logger.info(f"ğŸ–¼ï¸ æ”¶é›†åˆ° {len(valid_images)} å¼ å›¾ç‰‡")
        return valid_images[:5]  # æœ€å¤šä¿ç•™5å¼ 
    
    def _search_weibo(self, keyword: str) -> List[Dict[str, str]]:
        """
        æœç´¢å¾®åšå†…å®¹ (ä½¿ç”¨æ–°çš„å¾®åšå®¢æˆ·ç«¯)
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨å¾®åšå®¢æˆ·ç«¯æœç´¢
            search_results = self.weibo_client.search_posts(
                keyword=keyword,
                max_results=self.max_search_results
            )
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for result in search_results:
                if result.get('text') and result.get('url'):
                    results.append({
                        'title': result['text'][:100] + "..." if len(result['text']) > 100 else result['text'],
                        'url': result['url'],
                        'source': 'weibo',
                        'full_text': result['text'],
                        'images': result.get('images', []),
                        'user_name': result.get('user_name', ''),
                        'publish_time': result.get('publish_time', ''),
                        'interaction_data': {
                            'attitude_count': result.get('attitude_count', ''),
                            'comment_count': result.get('comment_count', ''),
                            'forward_count': result.get('forward_count', '')
                        }
                    })
            
            self.logger.info(f"ğŸ” å¾®åšæœç´¢å®Œæˆ: {keyword}, æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšæœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°åŸæœ‰æœç´¢æ–¹æ³•
            return self._search_weibo_fallback(keyword)
    
    def _search_weibo_fallback(self, keyword: str) -> List[Dict[str, str]]:
        """
        å¾®åšæœç´¢é™çº§æ–¹æ³•ï¼ˆæ— Cookieæ—¶ä½¿ç”¨ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            search_url = f"https://s.weibo.com/weibo"
            params = {
                'q': keyword,
                'sort': 'hot',
                'page': 1
            }
            
            response = requests.get(
                search_url, 
                params=params,
                headers=self.headers, 
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç´¢ç»“æœ
            cards = soup.select('.card-wrap')
            for card in cards[:self.max_search_results]:
                title_elem = card.select_one('.txt')
                link_elem = card.select_one('a[href*="/status/"]')
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = urljoin('https://weibo.com', link_elem['href'])
                    
                    results.append({
                        'title': title,
                        'url': url,
                        'source': 'weibo'
                    })
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšé™çº§æœç´¢å¤±è´¥: {e}")
            return []
        """
        æœç´¢ç™¾åº¦å†…å®¹
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            search_url = "https://www.baidu.com/s"
            params = {
                'wd': keyword,
                'rn': 10,
                'tn': 'baiduhome_pg'
            }
            
            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£æç™¾åº¦æœç´¢ç»“æœ
            for item in soup.select('.result.c-container')[:self.max_search_results]:
                title_elem = item.select_one('h3 a')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    if url:
                        results.append({
                            'title': title,
                            'url': url,
                            'source': 'baidu'
                        })
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_sogou(self, keyword: str) -> List[Dict[str, str]]:
        """
        æœç´¢æœç‹—å†…å®¹
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            search_url = "https://www.sogou.com/web"
            params = {
                'query': keyword,
                'num': 10
            }
            
            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç‹—æœç´¢ç»“æœ
            for item in soup.select('.result')[:self.max_search_results]:
                title_elem = item.select_one('h3 a')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    if url:
                        results.append({
                            'title': title,
                            'url': url,
                            'source': 'sogou'
                        })
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æœç‹—æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_weibo_images(self, keyword: str) -> List[str]:
        """
        æœç´¢å¾®åšå›¾ç‰‡ (ä½¿ç”¨æ–°çš„å¾®åšå®¢æˆ·ç«¯)
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨å¾®åšå®¢æˆ·ç«¯æœç´¢å›¾ç‰‡
            image_urls = self.weibo_client.search_images(
                keyword=keyword,
                max_results=10
            )
            
            self.logger.info(f"ğŸ–¼ï¸ å¾®åšå›¾ç‰‡æœç´¢å®Œæˆ: {keyword}, æ‰¾åˆ° {len(image_urls)} å¼ å›¾ç‰‡")
            return image_urls
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšå›¾ç‰‡æœç´¢å¤±è´¥: {e}")
            return []
        
    
    def _extract_text_from_url(self, url: str) -> Optional[str]:
        """
        ä»URLæå–æ–‡æœ¬å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            if not url or not self._is_valid_url(url):
                return None
            
            response = requests.get(
                url,
                headers=self.headers,
                timeout=self.request_timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–ä¸»è¦å†…å®¹
            content_selectors = [
                '.content', '.article', '.post', '.text',
                'article', 'main', '.main-content'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(separator='\n', strip=True)
                    break
            
            if not content:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–bodyæ–‡æœ¬
                body = soup.find('body')
                if body:
                    content = body.get_text(separator='\n', strip=True)
            
            # æ¸…ç†å’Œæˆªå–
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            clean_content = '\n'.join(lines)
            
            # é™åˆ¶é•¿åº¦
            if len(clean_content) > 2000:
                clean_content = clean_content[:2000] + "..."
            
            return clean_content if len(clean_content) > 50 else None
            
        except Exception as e:
            self.logger.debug(f"âŒ æå–ç½‘é¡µå†…å®¹å¤±è´¥: {url}, {e}")
            return None
    
    def _is_relevant_content(self, title: str, content: str) -> bool:
        """
        æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸æ ‡é¢˜ç›¸å…³
        
        Args:
            title: äº‹ä»¶æ ‡é¢˜
            content: ç½‘é¡µå†…å®¹
            
        Returns:
            æ˜¯å¦ç›¸å…³
        """
        try:
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ¤æ–­
            similarity = self.vector_utils.text_similarity(title, content[:500])  # åªæ¯”è¾ƒå‰500å­—ç¬¦
            
            is_relevant = similarity >= self.similarity_threshold
            self.logger.debug(f"ğŸ“Š å†…å®¹ç›¸å…³æ€§: {similarity:.3f}, é˜ˆå€¼: {self.similarity_threshold}, ç›¸å…³: {is_relevant}")
            
            return is_relevant
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç›¸å…³æ€§æ£€æŸ¥å¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå…³é”®è¯åŒ¹é…
            return self._keyword_relevance_check(title, content)
    
    def _keyword_relevance_check(self, title: str, content: str) -> bool:
        """
        å…³é”®è¯ç›¸å…³æ€§æ£€æŸ¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            title: äº‹ä»¶æ ‡é¢˜
            content: ç½‘é¡µå†…å®¹
            
        Returns:
            æ˜¯å¦ç›¸å…³
        """
        # æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯
        title_words = set(title.replace(' ', ''))
        content_words = set(content.replace(' ', ''))
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        intersection = title_words.intersection(content_words)
        if len(title_words) > 0:
            relevance_ratio = len(intersection) / len(title_words)
            return relevance_ratio >= 0.3  # è‡³å°‘30%çš„å…³é”®è¯åŒ¹é…
        
        return False
    
    def _deduplicate_texts(self, texts: List[Dict[str, Any]]) -> List[str]:
        """
        å»é‡æ–‡æœ¬ç´ æ
        
        Args:
            texts: æ–‡æœ¬ç´ æåˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not texts:
            return []
        
        unique_texts = []
        seen_contents = set()
        
        for text_item in texts:
            content = text_item.get('content', '')
            if not content:
                continue
            
            # ç®€å•çš„é‡å¤æ£€æŸ¥
            content_hash = hash(content[:200])  # ä½¿ç”¨å‰200å­—ç¬¦çš„å“ˆå¸Œ
            
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_texts.append(content)
        
        return unique_texts
    
    def _validate_image_urls(self, image_urls: List[str]) -> List[str]:
        """
        éªŒè¯å›¾ç‰‡URLçš„æœ‰æ•ˆæ€§
        
        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            
        Returns:
            æœ‰æ•ˆçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        valid_urls = []
        
        for url in image_urls:
            try:
                if self._is_valid_image_url(url):
                    valid_urls.append(url)
                    
            except Exception as e:
                self.logger.debug(f"âŒ å›¾ç‰‡URLéªŒè¯å¤±è´¥: {url}, {e}")
                continue
        
        return valid_urls
    
    def _is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def _is_valid_image_url(self, url: str) -> bool:
        """æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ"""
        if not self._is_valid_url(url):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        valid_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.webp')
        url_lower = url.lower()
        
        return any(ext in url_lower for ext in valid_extensions) or 'sinaimg' in url_lower
    
    def _mark_processing(self, event_id: str):
        """æ ‡è®°äº‹ä»¶ä¸ºå¤„ç†ä¸­"""
        try:
            self.es.update_by_id(
                index=self.index_name,
                doc_id=event_id,
                doc={"material_collected": False}
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ ‡è®°å¤„ç†çŠ¶æ€å¤±è´¥: {event_id}, {e}")
    
    def _mark_failed(self, event_id: str):
        """æ ‡è®°äº‹ä»¶å¤„ç†å¤±è´¥"""
        try:
            self.es.update_by_id(
                index=self.index_name,
                doc_id=event_id,
                doc={
                    "material_collected": True,
                    "material_collection_failed": True
                }
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ ‡è®°å¤±è´¥çŠ¶æ€å¤±è´¥: {event_id}, {e}")
    
    def _update_event_materials(self, event_id: str, materials: Dict[str, Any]) -> bool:
        """
        æ›´æ–°äº‹ä»¶çš„ç´ æä¿¡æ¯
        
        Args:
            event_id: äº‹ä»¶ID
            materials: ç´ ææ•°æ®
            
        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            update_data = {
                "material": materials,
                "material_collected": True
            }
            
            success = self.es.update_by_id(
                index=self.index_name,
                doc_id=event_id,
                doc=update_data
            )
            
            if success:
                text_count = len(materials.get('texts', []))
                image_count = len(materials.get('image_urls', []))
                self.logger.info(f"âœ… ç´ ææ›´æ–°æˆåŠŸ: {event_id}, æ–‡æœ¬:{text_count}, å›¾ç‰‡:{image_count}")
                return True
            else:
                self.logger.error(f"âŒ ç´ ææ›´æ–°å¤±è´¥: {event_id}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°ç´ æå¼‚å¸¸: {event_id}, {e}")
            return False