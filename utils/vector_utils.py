import os
import logging
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Union
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
from functools import lru_cache
from dotenv import load_dotenv

load_dotenv('config/.env')

class VectorUtils:
    """
    æ–‡æœ¬å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—å·¥å…·ç±»
    æ”¯æŒå¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ã€æ‰¹é‡å¤„ç†å’Œç¼“å­˜ä¼˜åŒ–
    """
    
    def __init__(self, 
                 model_name_or_path: Optional[str] = None,
                 device: Optional[str] = None,
                 cache_size: int = 1000):
        """
        åˆå§‹åŒ–å‘é‡å·¥å…·ç±»
        
        Args:
            model_name_or_path: æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cpu', 'cuda', 'auto')
            cache_size: å‘é‡ç¼“å­˜å¤§å°
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # é…ç½®å‚æ•°
        self.model_name = model_name_or_path or os.getenv('VECTOR_MODEL', 'BAAI/bge-m3')
        self.device = self._get_device(device)
        self.cache_size = cache_size
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.dimension = None
        self._load_model()
        
        # å‘é‡ç¼“å­˜
        self._vector_cache = {}
        
    def _get_device(self, device: Optional[str]) -> str:
        """ç¡®å®šè®¡ç®—è®¾å¤‡"""
        if device == 'auto' or device is None:
            if torch.cuda.is_available():
                device = 'cuda'
                self.logger.info("ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                device = 'cpu'
                self.logger.info("ğŸ’» ä½¿ç”¨CPUè®¡ç®—")
        return device
    
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            self.logger.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(
                self.model_name,
                device=self.device,
                trust_remote_code=True
            )
            
            # è·å–å‘é‡ç»´åº¦
            test_text = "test"
            test_embedding = self.model.encode(test_text)
            self.dimension = len(test_embedding)
            
            self.logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
            self.logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {self.dimension}, è®¾å¤‡: {self.device}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.logger.error("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
            raise
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _manage_cache(self):
        """ç®¡ç†ç¼“å­˜å¤§å°"""
        if len(self._vector_cache) > self.cache_size:
            # æ¸…ç†æœ€æ—§çš„ç¼“å­˜é¡¹
            keys_to_remove = list(self._vector_cache.keys())[:-self.cache_size//2]
            for key in keys_to_remove:
                del self._vector_cache[key]
            self.logger.debug(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(self._vector_cache)} é¡¹")
    
    def embed(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        if not self.model:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        if not text or not text.strip():
            self.logger.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros(self.dimension)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cache_key = self._get_cache_key(text)
            if cache_key in self._vector_cache:
                return self._vector_cache[cache_key]
        
        try:
            # ç”Ÿæˆå‘é‡
            embedding = self.model.encode(text, convert_to_tensor=False)
            embedding = np.array(embedding, dtype=np.float32)
            
            # å­˜å…¥ç¼“å­˜
            if use_cache:
                self._vector_cache[cache_key] = embedding
                self._manage_cache()
            
            return embedding
            
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return np.zeros(self.dimension)
    
    def embed_batch(self, 
                    texts: List[str], 
                    batch_size: int = 32,
                    show_progress: bool = False) -> List[np.ndarray]:
        """
        æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        if not self.model:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        if not texts:
            return []
        
        try:
            # é¢„å¤„ç†ï¼šè¿‡æ»¤ç©ºæ–‡æœ¬
            valid_texts = []
            text_indices = []
            
            for i, text in enumerate(texts):
                if text and text.strip():
                    valid_texts.append(text)
                    text_indices.append(i)
            
            if not valid_texts:
                self.logger.warning("âš ï¸ æ‰€æœ‰è¾“å…¥æ–‡æœ¬ä¸ºç©º")
                return [np.zeros(self.dimension) for _ in texts]
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡
            self.logger.info(f"ğŸ”„ æ‰¹é‡å¤„ç† {len(valid_texts)} ä¸ªæ–‡æœ¬")
            
            embeddings = self.model.encode(
                valid_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_tensor=False
            )
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            embeddings = np.array(embeddings, dtype=np.float32)
            
            # æ„å»ºå®Œæ•´ç»“æœåˆ—è¡¨
            results = []
            valid_idx = 0
            
            for i in range(len(texts)):
                if i in text_indices:
                    results.append(embeddings[valid_idx])
                    valid_idx += 1
                else:
                    results.append(np.zeros(self.dimension))
            
            self.logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return [np.zeros(self.dimension) for _ in texts]
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
            
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å€¼ (-1 åˆ° 1)
        """
        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(np.clip(similarity, -1.0, 1.0))
            
        except Exception as e:
            self.logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦å€¼ (0 åˆ° 1)
        """
        vec1 = self.embed(text1)
        vec2 = self.embed(text2)
        similarity = self.cosine_similarity(vec1, vec2)
        
        # è½¬æ¢åˆ°0-1èŒƒå›´
        return (similarity + 1) / 2
    
    def find_most_similar(self, 
                         query: str,
                         candidates: List[str],
                         top_k: int = 5,
                         threshold: float = 0.0) -> List[Dict[str, Any]]:
        """
        æ‰¾åˆ°ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ
            threshold: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸ä¼¼åº¦ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not query or not candidates:
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vec = self.embed(query)
            
            # æ‰¹é‡ç”Ÿæˆå€™é€‰å‘é‡
            candidate_vecs = self.embed_batch(candidates)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for i, candidate_vec in enumerate(candidate_vecs):
                similarity = self.cosine_similarity(query_vec, candidate_vec)
                
                if similarity >= threshold:
                    similarities.append({
                        'text': candidates[i],
                        'similarity': similarity,
                        'index': i
                    })
            
            # æ’åºå¹¶è¿”å›top_k
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            result = similarities[:top_k]
            self.logger.debug(f"ğŸ” ç›¸ä¼¼åº¦æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:50]}...', æ‰¾åˆ° {len(result)} ä¸ªç»“æœ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def top_k_indices(self, 
                     query_vec: np.ndarray,
                     candidate_vecs: List[np.ndarray],
                     k: int) -> List[int]:
        """
        è¿”å›ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„å‰kä¸ªå€™é€‰å‘é‡çš„ç´¢å¼•
        
        Args:
            query_vec: æŸ¥è¯¢å‘é‡
            candidate_vecs: å€™é€‰å‘é‡åˆ—è¡¨
            k: è¿”å›æ•°é‡
            
        Returns:
            ç›¸ä¼¼åº¦æœ€é«˜çš„å‰kä¸ªç´¢å¼•åˆ—è¡¨
        """
        try:
            similarities = []
            for i, candidate_vec in enumerate(candidate_vecs):
                similarity = self.cosine_similarity(query_vec, candidate_vec)
                similarities.append((similarity, i))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[0], reverse=True)
            
            # è¿”å›å‰kä¸ªç´¢å¼•
            return [idx for _, idx in similarities[:k]]
            
        except Exception as e:
            self.logger.error(f"âŒ Top-kç´¢å¼•è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def cluster_texts(self, 
                     texts: List[str], 
                     threshold: float = 0.8,
                     min_cluster_size: int = 2) -> List[List[int]]:
        """
        åŸºäºç›¸ä¼¼åº¦å¯¹æ–‡æœ¬è¿›è¡Œèšç±»
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            threshold: èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼
            min_cluster_size: æœ€å°èšç±»å¤§å°
            
        Returns:
            èšç±»ç»“æœï¼Œæ¯ä¸ªèšç±»åŒ…å«æ–‡æœ¬ç´¢å¼•åˆ—è¡¨
        """
        if len(texts) < min_cluster_size:
            return []
        
        try:
            # ç”Ÿæˆå‘é‡
            vectors = self.embed_batch(texts)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            n = len(vectors)
            similarity_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i+1, n):
                    sim = self.cosine_similarity(vectors[i], vectors[j])
                    similarity_matrix[i][j] = sim
                    similarity_matrix[j][i] = sim
            
            # ç®€å•èšç±»ç®—æ³•
            clusters = []
            used = set()
            
            for i in range(n):
                if i in used:
                    continue
                
                cluster = [i]
                used.add(i)
                
                for j in range(i+1, n):
                    if j not in used and similarity_matrix[i][j] >= threshold:
                        cluster.append(j)
                        used.add(j)
                
                if len(cluster) >= min_cluster_size:
                    clusters.append(cluster)
            
            self.logger.info(f"ğŸ“Š æ–‡æœ¬èšç±»å®Œæˆ: {len(texts)} ä¸ªæ–‡æœ¬åˆ†ä¸º {len(clusters)} ä¸ªèšç±»")
            return clusters
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æœ¬èšç±»å¤±è´¥: {e}")
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'device': self.device,
            'cache_size': len(self._vector_cache),
            'max_cache_size': self.cache_size
        }
    
    def clear_cache(self):
        """æ¸…ç©ºå‘é‡ç¼“å­˜"""
        self._vector_cache.clear()
        self.logger.info("ğŸ§¹ å‘é‡ç¼“å­˜å·²æ¸…ç©º")