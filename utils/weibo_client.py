import os
import time
import logging
import requests
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv('config/.env')

class WeiboClient:
    """
    å¾®åšå®¢æˆ·ç«¯å·¥å…·ç±»
    å¤„ç†å¾®åšCookieè®¤è¯å’Œå†…å®¹æŠ“å–
    """
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.cookie = os.getenv('WEIBO_COOKIE', '')
        self.user_agent = os.getenv('WEIBO_USER_AGENT', 
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        self.api_delay = float(os.getenv('WEIBO_API_DELAY', 2))
        self.max_retries = int(os.getenv('WEIBO_MAX_RETRIES', 3))
        self.request_timeout = int(os.getenv('REQUEST_TIMEOUT', 10))
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self._setup_session()
        
        # éªŒè¯Cookie
        if not self.cookie:
            self.logger.warning("âš ï¸ å¾®åšCookieæœªé…ç½®ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")
        else:
            self.logger.info("âœ… å¾®åšå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # è®¾ç½®Cookie
        if self.cookie:
            self.session.headers['Cookie'] = self.cookie
    
    def verify_cookie(self) -> bool:
        """
        éªŒè¯Cookieæ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            Cookieæ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # å°è¯•è®¿é—®å¾®åšä¸»é¡µ
            response = self.session.get(
                'https://weibo.com',
                timeout=self.request_timeout
            )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if 'ç™»å½•' in response.text or 'login' in response.url.lower():
                self.logger.error("âŒ å¾®åšCookieå·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°è·å–")
                return False
            
            self.logger.info("âœ… å¾®åšCookieéªŒè¯æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CookieéªŒè¯å¤±è´¥: {e}")
            return False
    
    def search_posts(self, keyword: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢å¾®åšå¸–å­
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            search_url = "https://s.weibo.com/weibo"
            params = {
                'q': keyword,
                'sort': 'hot',
                'page': 1
            }
            
            self.logger.info(f"ğŸ” æœç´¢å¾®åš: {keyword}")
            
            response = self.session.get(
                search_url,
                params=params,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            # è§£ææœç´¢ç»“æœ
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # æŸ¥æ‰¾å¾®åšå¡ç‰‡
            cards = soup.select('.card-wrap')
            
            for card in cards[:max_results]:
                try:
                    result = self._parse_search_card(card)
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.debug(f"è§£æå¡ç‰‡å¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            
            # APIè°ƒç”¨å»¶è¿Ÿ
            time.sleep(self.api_delay)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšæœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_search_card(self, card) -> Optional[Dict[str, Any]]:
        """
        è§£ææœç´¢ç»“æœå¡ç‰‡
        
        Args:
            card: BeautifulSoupå¡ç‰‡å…ƒç´ 
            
        Returns:
            è§£æåçš„ç»“æœå­—å…¸
        """
        try:
            result = {}
            
            # æå–æ–‡æœ¬å†…å®¹
            text_elem = card.select_one('.txt')
            if text_elem:
                # ç§»é™¤å¤šä½™çš„HTMLæ ‡ç­¾
                for tag in text_elem(['a', 'span']):
                    tag.decompose()
                result['text'] = text_elem.get_text(strip=True)
            
            # æå–é“¾æ¥
            link_elem = card.select_one('a[href*="/status/"]')
            if link_elem:
                href = link_elem.get('href', '')
                if href.startswith('/'):
                    href = 'https://weibo.com' + href
                result['url'] = href
                
                # ä»URLæå–å¾®åšID
                if '/status/' in href:
                    status_id = href.split('/status/')[-1].split('?')[0]
                    result['status_id'] = status_id
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            user_elem = card.select_one('.name')
            if user_elem:
                result['user_name'] = user_elem.get_text(strip=True)
            
            # æå–æ—¶é—´ä¿¡æ¯
            time_elem = card.select_one('.from')
            if time_elem:
                result['publish_time'] = time_elem.get_text(strip=True)
            
            # æå–äº’åŠ¨æ•°æ®
            attitude_elem = card.select_one('.card-act .attitude')
            if attitude_elem:
                result['attitude_count'] = attitude_elem.get_text(strip=True)
            
            comment_elem = card.select_one('.card-act .comment')
            if comment_elem:
                result['comment_count'] = comment_elem.get_text(strip=True)
            
            forward_elem = card.select_one('.card-act .forward')
            if forward_elem:
                result['forward_count'] = forward_elem.get_text(strip=True)
            
            # æå–å›¾ç‰‡
            images = []
            img_elems = card.select('img[src*="sinaimg"]')
            for img in img_elems:
                src = img.get('src', '')
                if src and 'sinaimg' in src:
                    # è½¬æ¢ä¸ºé«˜æ¸…å›¾ç‰‡URL
                    if 'thumbnail' in src:
                        src = src.replace('thumbnail', 'large')
                    images.append(src)
            
            result['images'] = images
            
            # åŸºæœ¬éªŒè¯
            if result.get('text') and result.get('url'):
                return result
            else:
                return None
                
        except Exception as e:
            self.logger.debug(f"è§£æå¡ç‰‡å¼‚å¸¸: {e}")
            return None
    
    def get_post_detail(self, status_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å¾®åšè¯¦æƒ…
        
        Args:
            status_id: å¾®åšçŠ¶æ€ID
            
        Returns:
            å¾®åšè¯¦æƒ…æ•°æ®
        """
        try:
            detail_url = f"https://weibo.com/status/{status_id}"
            
            response = self.session.get(
                detail_url,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„è§£æé€»è¾‘
            # ç”±äºå¾®åšé¡µé¢ç»“æ„å¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
            
            detail = {
                'status_id': status_id,
                'url': detail_url,
                'raw_html': response.text[:1000]  # ä¿ç•™éƒ¨åˆ†åŸå§‹HTMLç”¨äºè°ƒè¯•
            }
            
            time.sleep(self.api_delay)
            return detail
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¾®åšè¯¦æƒ…å¤±è´¥: {status_id}, {e}")
            return None
    
    def search_images(self, keyword: str, max_results: int = 10) -> List[str]:
        """
        æœç´¢å¾®åšå›¾ç‰‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        try:
            search_url = "https://s.weibo.com/pic"
            params = {'q': keyword}
            
            response = self.session.get(
                search_url,
                params=params,
                timeout=self.request_timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            image_urls = []
            
            # æå–å›¾ç‰‡URL
            img_elems = soup.select('img[src*="sinaimg"]')
            for img in img_elems[:max_results]:
                src = img.get('src', '')
                if src and 'sinaimg' in src:
                    # è½¬æ¢ä¸ºé«˜æ¸…å›¾ç‰‡URL
                    if 'thumbnail' in src:
                        src = src.replace('thumbnail', 'large')
                    image_urls.append(src)
            
            # å»é‡
            unique_images = list(set(image_urls))
            
            self.logger.info(f"ğŸ–¼ï¸ æœç´¢åˆ° {len(unique_images)} å¼ å›¾ç‰‡")
            
            time.sleep(self.api_delay)
            return unique_images
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšå›¾ç‰‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_cookie_info(self) -> Dict[str, Any]:
        """
        è·å–Cookieä¿¡æ¯å’ŒçŠ¶æ€
        
        Returns:
            CookieçŠ¶æ€ä¿¡æ¯
        """
        cookie_info = {
            'has_cookie': bool(self.cookie),
            'cookie_length': len(self.cookie) if self.cookie else 0,
            'is_valid': False,
            'user_agent': self.user_agent,
            'api_delay': self.api_delay
        }
        
        if self.cookie:
            cookie_info['is_valid'] = self.verify_cookie()
            
            # è§£æCookieä¸­çš„ç”¨æˆ·ä¿¡æ¯
            if 'SUB=' in self.cookie:
                cookie_info['has_login_token'] = True
            
        return cookie_info
    
    def update_cookie(self, new_cookie: str) -> bool:
        """
        æ›´æ–°Cookie
        
        Args:
            new_cookie: æ–°çš„Cookieå­—ç¬¦ä¸²
            
        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            self.cookie = new_cookie
            self.session.headers['Cookie'] = new_cookie
            
            # éªŒè¯æ–°Cookie
            if self.verify_cookie():
                self.logger.info("âœ… Cookieæ›´æ–°æˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ æ–°CookieéªŒè¯å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Cookieæ›´æ–°å¼‚å¸¸: {e}")
            return False