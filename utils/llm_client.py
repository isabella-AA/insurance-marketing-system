import os
import json
import time
import logging
import requests
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv('config/.env')

@dataclass
class LLMResponse:
    """LLMå“åº”ç»“æœå°è£…"""
    success: bool
    content: str
    error: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    model: Optional[str] = None

class LLMError(Exception):
    """LLMè°ƒç”¨å¼‚å¸¸"""
    def __init__(self, message: str, error_type: str = "unknown"):
        super().__init__(message)
        self.error_type = error_type

class GLMClient:
    """
    æ™ºè°±GLMå¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯å°è£…
    æ”¯æŒå¤šç§è°ƒç”¨æ–¹å¼ã€é‡è¯•æœºåˆ¶å’Œå®Œæ•´çš„å¼‚å¸¸å¤„ç†
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–GLMå®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.api_key = api_key or os.getenv('GLM_API_KEY')
        if not self.api_key:
            raise ValueError("GLM_API_KEY æœªè®¾ç½®")
        
        # é…ç½®å‚æ•°
        self.base_url = os.getenv('GLM_BASE_URL', 'https://open.bigmodel.cn/api/paas/v4/chat/completions')
        self.default_model = os.getenv('GLM_MODEL', 'glm-4-air')
        self.timeout = int(os.getenv('GLM_TIMEOUT', 60))
        self.max_retries = int(os.getenv('GLM_MAX_RETRIES', 3))
        self.retry_delay = float(os.getenv('GLM_RETRY_DELAY', 1.0))
        
        # è¯·æ±‚å¤´
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        self.logger.info(f"âœ… GLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {self.default_model}")
    
    def chat(self, 
             user_input: str,
             system_prompt: Optional[str] = None,
             model: Optional[str] = None,
             temperature: float = 0.7,
             max_tokens: Optional[int] = None,
             messages: Optional[List[Dict[str, str]]] = None) -> LLMResponse:
        """
        èŠå¤©å¯¹è¯æ¥å£
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥user_inputå’Œsystem_prompt
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            if messages is None:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": user_input})
            
            # æ„å»ºè¯·æ±‚ä½“
            payload = {
                "model": model or self.default_model,
                "messages": messages,
                "temperature": temperature
            }
            
            if max_tokens:
                payload["max_tokens"] = max_tokens
            
            # å‘é€è¯·æ±‚
            response = self._make_request(payload)
            return self._parse_response(response)
            
        except Exception as e:
            self.logger.error(f"âŒ èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
            return LLMResponse(
                success=False,
                content="",
                error=str(e)
            )
    
    def simple_chat(self, user_input: str, system_prompt: Optional[str] = None) -> str:
        """
        ç®€åŒ–çš„èŠå¤©æ¥å£ï¼Œç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        response = self.chat(user_input=user_input, system_prompt=system_prompt)
        return response.content if response.success else ""
    
    def extract_json(self, 
                     user_input: str, 
                     system_prompt: Optional[str] = None,
                     expected_keys: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        æå–JSONæ ¼å¼çš„å“åº”
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            expected_keys: æœŸæœ›çš„JSONé”®ååˆ—è¡¨ï¼Œç”¨äºéªŒè¯
            
        Returns:
            è§£æåçš„JSONå¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç³»ç»Ÿæç¤ºè¯ï¼Œæ·»åŠ JSONæ ¼å¼è¦æ±‚
        if not system_prompt:
            system_prompt = "è¯·ä»¥JSONæ ¼å¼å›å¤ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚"
        elif "json" not in system_prompt.lower():
            system_prompt += "\nè¯·ä»¥JSONæ ¼å¼å›å¤ã€‚"
        
        response = self.chat(user_input=user_input, system_prompt=system_prompt)
        
        if not response.success:
            self.logger.error(f"âŒ JSONæå–å¤±è´¥: {response.error}")
            return {}
        
        try:
            # å°è¯•è§£æJSON
            result = json.loads(response.content)
            
            # éªŒè¯æœŸæœ›çš„é”®å
            if expected_keys:
                missing_keys = [key for key in expected_keys if key not in result]
                if missing_keys:
                    self.logger.warning(f"âš ï¸ JSONç¼ºå°‘æœŸæœ›çš„é”®: {missing_keys}")
            
            self.logger.debug(f"âœ… JSONè§£ææˆåŠŸ: {result}")
            return result
            
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            self.logger.error(f"ğŸ“‹ åŸå§‹å›å¤: {response.content}")
            return {}
    
    def batch_chat(self, 
                   inputs: List[Dict[str, Any]], 
                   delay: float = 0.5) -> List[LLMResponse]:
        """
        æ‰¹é‡èŠå¤©è¯·æ±‚
        
        Args:
            inputs: è¾“å…¥åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«chatæ–¹æ³•çš„å‚æ•°
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            
        Returns:
            å“åº”ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, input_params in enumerate(inputs):
            self.logger.info(f"ğŸ“¤ å¤„ç†æ‰¹é‡è¯·æ±‚ {i+1}/{len(inputs)}")
            
            response = self.chat(**input_params)
            results.append(response)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
            if delay > 0 and i < len(inputs) - 1:
                time.sleep(delay)
        
        success_count = sum(1 for r in results if r.success)
        self.logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(inputs)} æˆåŠŸ")
        
        return results
    
    def _make_request(self, payload: Dict[str, Any]) -> requests.Response:
        """
        å‘é€HTTPè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
        
        Args:
            payload: è¯·æ±‚ä½“
            
        Returns:
            HTTPå“åº”å¯¹è±¡
        """
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"ğŸ“¤ å‘é€GLMè¯·æ±‚ (å°è¯• {attempt + 1}/{self.max_retries})")
                self.logger.debug(f"ğŸ“‹ è¯·æ±‚å†…å®¹: {json.dumps(payload, ensure_ascii=False, indent=2)}")
                
                response = requests.post(
                    self.base_url,
                    headers=self.headers,
                    json=payload,
                    timeout=self.timeout
                )
                
                response.raise_for_status()
                return response
                
            except requests.exceptions.Timeout as e:
                last_error = LLMError(f"è¯·æ±‚è¶…æ—¶: {e}", "timeout")
                self.logger.warning(f"â° è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                
            except requests.exceptions.ConnectionError as e:
                last_error = LLMError(f"è¿æ¥é”™è¯¯: {e}", "connection")
                self.logger.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                
            except requests.exceptions.HTTPError as e:
                response_text = getattr(e.response, 'text', 'Unknown error')
                last_error = LLMError(f"HTTPé”™è¯¯: {e}, å“åº”: {response_text}", "http")
                self.logger.error(f"âŒ HTTPé”™è¯¯: {e}")
                # HTTPé”™è¯¯é€šå¸¸ä¸éœ€è¦é‡è¯•
                break
                
            except Exception as e:
                last_error = LLMError(f"æœªçŸ¥é”™è¯¯: {e}", "unknown")
                self.logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            
            # é‡è¯•å»¶è¿Ÿ
            if attempt < self.max_retries - 1:
                delay = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                self.logger.debug(f"ğŸ˜´ ç­‰å¾… {delay:.1f}s åé‡è¯•...")
                time.sleep(delay)
        
        raise last_error
    
    def _parse_response(self, response: requests.Response) -> LLMResponse:
        """
        è§£æAPIå“åº”
        
        Args:
            response: HTTPå“åº”å¯¹è±¡
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        try:
            data = response.json()
            
            if "choices" not in data or not data["choices"]:
                raise LLMError("å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ", "format")
            
            content = data["choices"][0]["message"]["content"]
            usage = data.get("usage", {})
            model = data.get("model")
            
            self.logger.debug(f"âœ… GLMå“åº”è§£ææˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
            self.logger.debug(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ: {usage}")
            
            return LLMResponse(
                success=True,
                content=content,
                usage=usage,
                model=model
            )
            
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ å“åº”JSONè§£æå¤±è´¥: {e}")
            return LLMResponse(
                success=False,
                content="",
                error=f"JSONè§£æå¤±è´¥: {e}"
            )
        except Exception as e:
            self.logger.error(f"âŒ å“åº”è§£æå¼‚å¸¸: {e}")
            return LLMResponse(
                success=False,
                content="",
                error=f"å“åº”è§£æå¼‚å¸¸: {e}"
            )
    
    def health_check(self) -> bool:
        """
        å¥åº·æ£€æŸ¥ï¼ŒéªŒè¯APIè¿æ¥æ˜¯å¦æ­£å¸¸
        
        Returns:
            è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            response = self.simple_chat("Hello", "è¯·ç®€å•å›å¤ä¸€ä¸ªå­—ã€‚")
            return bool(response.strip())
        except Exception as e:
            self.logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False